{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bad SQL Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.23.3\n",
      "pandas version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sys.path.append('..')\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "# ----------------infos----------------------\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "# np.show_config()\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "from evaluate import evaluate_all\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chency/PythonProjects/balance/bad_sql_experiments\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 读取训练数据， original from OSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 96\n"
     ]
    }
   ],
   "source": [
    "csv_dir_path = \"bad_sql_data/\"\n",
    "record = pd.read_csv(os.path.join(csv_dir_path, 'record.csv'))\n",
    "sql_ = pd.read_csv(os.path.join(csv_dir_path, 'sql.csv'))\n",
    "metric = pd.read_csv(os.path.join(csv_dir_path, 'metric.csv'))\n",
    "\n",
    "record['occur_time'] = pd.to_datetime(record['occur_time'])\n",
    "sql_['batch_time'] = pd.to_datetime(sql_['batch_time'])\n",
    "metric['event_date'] = pd.to_datetime(metric['event_date'])\n",
    "\n",
    "record_filtered = record.loc[(record.occur_time > '2021/6/2')]\n",
    "record_filtered = record_filtered.sort_values('occur_time')\n",
    "clusters = record_filtered.cluster\n",
    "timestamps = record_filtered.occur_time\n",
    "tenants = record_filtered.tenant_name\n",
    "labels = record_filtered.sql_id\n",
    "\n",
    "print(len(record_filtered), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluate BMFS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_BMFS(metric, sql_, clusters, tenants, timestamps, labels, after=1):\n",
    "    result_rows = []\n",
    "    for impute in [False]:\n",
    "        for positive in [True]:\n",
    "            for normalize in ['allstd']:\n",
    "                for attribution_method in ['gradients_x_inputs',]:\n",
    "                    hit_num, miss_num, _, _, avg_rec_num, avg_time = evaluate_all(metric, sql_, clusters, tenants, timestamps, labels, after=after, trainer='bayesFS', attribution_method=attribution_method, impute=impute, positive=positive, normalize=normalize)\n",
    "                    hit_rate = hit_num / (hit_num + miss_num)\n",
    "                    row = {'impute': impute,\n",
    "                          'positive': positive,\n",
    "                          'normalize': normalize,\n",
    "                          'attribution_method': attribution_method,\n",
    "                          'Accuracy(top3)': hit_rate,\n",
    "                           'avg_rec_num': avg_rec_num,\n",
    "                           \"avg_time\": avg_time}\n",
    "                    result_rows.append(row)\n",
    "    return pd.DataFrame(result_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_df = evaluate_BMFS(metric, sql_, clusters, tenants, timestamps, labels, after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   impute  positive normalize  attribution_method  Accuracy(top3)  \\\n0   False      True    allstd  gradients_x_inputs        0.833333   \n\n   avg_rec_num  avg_time  \n0     2.277778  1.121089  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>impute</th>\n      <th>positive</th>\n      <th>normalize</th>\n      <th>attribution_method</th>\n      <th>Accuracy(top3)</th>\n      <th>avg_rec_num</th>\n      <th>avg_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>True</td>\n      <td>allstd</td>\n      <td>gradients_x_inputs</td>\n      <td>0.833333</td>\n      <td>2.277778</td>\n      <td>1.121089</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluate fsMTS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def evaluate_fsMTS(metric, sql_, clusters, tenants, timestamps, labels, after=1):\n",
    "    result_rows = []\n",
    "    for impute in [True]:\n",
    "        for positive in [True]:\n",
    "            for normalize in ['allstd']:\n",
    "                for attribution_method in ['gradients']:\n",
    "                    hit_num, miss_num, _, _, avg_rec_num, avg_time = evaluate_all(metric, sql_, clusters, tenants, timestamps, labels, after=after, trainer='fsMTS', attribution_method=attribution_method, impute=impute, positive=positive, normalize=normalize, leftshit=False)\n",
    "                    hit_rate = hit_num / (hit_num + miss_num)\n",
    "                    row = {'impute': impute,\n",
    "                          'positive': positive,\n",
    "                          'normalize': normalize,\n",
    "                          'attribution_method': attribution_method,\n",
    "                          'Accuracy(top3)': hit_rate,\n",
    "                           'avg_rec_num': avg_rec_num,\n",
    "                           \"avg_time\": avg_time}\n",
    "                    result_rows.append(row)\n",
    "    return pd.DataFrame(result_rows)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df = evaluate_fsMTS(metric, sql_, clusters, tenants, timestamps, labels, after=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluate enet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_enet(metric, sql_, clusters, tenants, timestamps, labels, after=1):\n",
    "    result_rows = []\n",
    "    for impute in [True]:\n",
    "        for positive in [True]:\n",
    "            for normalize in ['allstd']:\n",
    "                for attribution_method in ['gradients_x_inputs']:\n",
    "                    hit_num, miss_num, _, _, avg_rec_num, avg_time = evaluate_all(metric, sql_, clusters, tenants, timestamps, labels, after=after, trainer='myenet', attribution_method=attribution_method, impute=impute, positive=positive, normalize=normalize)\n",
    "                    hit_rate = hit_num / (hit_num + miss_num)\n",
    "                    row = {'impute': impute,\n",
    "                          'positive': positive,\n",
    "                          'normalize': normalize,\n",
    "                          'attribution_method': attribution_method,\n",
    "                          'Accuracy(top3)': hit_rate,\n",
    "                           'avg_rec_num': avg_rec_num,\n",
    "                           \"avg_time\": avg_time}\n",
    "                    result_rows.append(row)\n",
    "    return pd.DataFrame(result_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_df = evaluate_enet(metric, sql_, clusters, tenants, timestamps, labels, after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluate ARD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def evaluate_ard(metric, sql_, clusters, tenants, timestamps, labels, after=1):\n",
    "    result_rows = []\n",
    "    for impute in [False]:\n",
    "        for positive in [True]:\n",
    "            for normalize in ['allstd']:\n",
    "                for attribution_method in ['gradients_x_inputs']:\n",
    "                    hit_num, miss_num, _, _, avg_rec_num, avg_time = evaluate_all(metric, sql_, clusters, tenants, timestamps, labels, after=after, trainer='myard', attribution_method=attribution_method, impute=impute, positive=positive, normalize=normalize)\n",
    "                    hit_rate = hit_num / (hit_num + miss_num)\n",
    "                    row = {'impute': impute,\n",
    "                          'positive': positive,\n",
    "                          'normalize': normalize,\n",
    "                          'attribution_method': attribution_method,\n",
    "                          'Accuracy(top3)': hit_rate,\n",
    "                           'avg_rec_num': avg_rec_num,\n",
    "                           \"avg_time\": avg_time}\n",
    "                    result_rows.append(row)\n",
    "    return pd.DataFrame(result_rows)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df = evaluate_ard(metric, sql_, clusters, tenants, timestamps, labels, after=1)\n",
    "result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluate LASSO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def evaluate_lasso(metric, sql_, clusters, tenants, timestamps, labels, after=1):\n",
    "    result_rows = []\n",
    "    for impute in [True]:\n",
    "        for positive in [True]:\n",
    "            for normalize in ['allstd']:\n",
    "                for attribution_method in ['gradients_x_inputs']:\n",
    "                    hit_num, miss_num, _, _, avg_rec_num, avg_time = evaluate_all(metric, sql_, clusters, tenants, timestamps, labels, after=after, trainer='mylasso', attribution_method=attribution_method, impute=impute, positive=positive, normalize=normalize)\n",
    "                    hit_rate = hit_num / (hit_num + miss_num)\n",
    "                    row = {'impute': impute,\n",
    "                          'positive': positive,\n",
    "                          'normalize': normalize,\n",
    "                          'attribution_method': attribution_method,\n",
    "                          'Accuracy(top3)': hit_rate,\n",
    "                           'avg_rec_num': avg_rec_num,\n",
    "                           \"avg_time\": avg_time}\n",
    "                    result_rows.append(row)\n",
    "    return pd.DataFrame(result_rows)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df = evaluate_lasso(metric, sql_, clusters, tenants, timestamps, labels, after=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.996px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}