{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, LinearRegression\n",
    "from collections import OrderedDict\n",
    "from trainer import bayesFS, my_lasso_train, my_enet_train, my_ard_train, fsMTS_train\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Get ground truth first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path_to_data = \"/Users/chency/PythonProjects/exathlon/\"\n",
    "truth = pd.read_csv(path_to_data + \"data/raw/ground_truth.csv\", dtype={\"root_cause_start\": int, \"root_cause_end\": int},na_filter=False)\n",
    "truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load cases\n",
    "each case contains a ground truth label and their multi time series from corresponding trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('cases.pickle'):\n",
    "    \"load existing cases data\"\n",
    "    with open('cases.pickle', 'rb') as f:\n",
    "        cases = pickle.load(f)\n",
    "else:\n",
    "    \"no cases saved yet, clean them from exathlon raw data\"\n",
    "    cases = []\n",
    "    for i, case in truth.iterrows():\n",
    "        start = case['root_cause_start']\n",
    "        end = case['root_cause_end']\n",
    "        start_min = pd.to_datetime(start, unit='s').floor('min')\n",
    "        end_min = pd.to_datetime(end, unit='s').floor('min')\n",
    "        trace_name = case['trace_name']\n",
    "        application = trace_name.split('_')[0]\n",
    "        tmp = case.to_dict()\n",
    "        tmp['start_min'] = start_min\n",
    "        tmp['end_min'] = end_min\n",
    "        metrics = pd.read_csv(path_to_data + \"data/raw/app{}/{}.csv\".format(application, trace_name))\n",
    "        metrics = metrics[(metrics.t <= end) & (metrics.t > end - 10800)]\n",
    "        metrics['time'] = pd.to_datetime(metrics['t'], unit='s')\n",
    "        metrics = metrics.groupby(pd.Grouper(key='time', freq='1min')).mean()\n",
    "        tmp['metrics'] = metrics\n",
    "        cases.append(tmp)\n",
    "    with open('cases.pickle', 'wb') as f:\n",
    "        pickle.dump(cases, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(case: dict, diff_columns, pre='30min', after='1min'):\n",
    "    \"\"\"\n",
    "    clip all metrics , Normalization\n",
    "    \"\"\"\n",
    "\n",
    "    start_min = case['start_min'].floor('min')\n",
    "    end_min = case['end_min'].floor('min')\n",
    "    print(\"start minute: {}, end minute: {}\".format(start_min, end_min))\n",
    "    metrics = case['metrics']\n",
    "\n",
    "\n",
    "    # clip [start_min - pre, start_min + after] time series\n",
    "    metrics = metrics.drop('t', axis=1)[start_min - pd.Timedelta(pre): start_min + pd.Timedelta(after)]\n",
    "    metrics = metrics.loc[:, metrics.isna().sum() != len(metrics)]\n",
    "\n",
    "    for col in metrics.columns:\n",
    "        for diff_col in diff_columns:\n",
    "            if col.endswith(diff_col):\n",
    "                metrics[col] = metrics[col].diff().bfill()\n",
    "                break\n",
    "\n",
    "    # standardrize, consider one-value array\n",
    "    m = metrics.mean(axis=0)\n",
    "    std = metrics.std()\n",
    "    std[std == 0] = 1\n",
    "    norm_metrics = (metrics - m) / std\n",
    "    \"\"\"\n",
    "    # m = test_m[:start_min - pd.Timedelta('1min')].mean(axis=0)\n",
    "    # std = test_m[:start_min - pd.Timedelta('1min')].std()\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # m = test_m - test_m[:start_min - pd.Timedelta('1min')].min(axis=0)\n",
    "    # minmax = test_m.max(axis=0) - test_m.min(axis=0)\n",
    "    # minmax[minmax == 0] = 1\n",
    "    # m = m / minmax\n",
    "    \"\"\"\n",
    "\n",
    "    return norm_metrics, start_min, end_min\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detrending(metrics: pd.DataFrame, start_min):\n",
    "    \"\"\"detrend normalized metrics\"\"\"\n",
    "    Y = metrics[:start_min - pd.Timedelta('1min')]\n",
    "    X = np.arange(len(Y)).reshape(-1, 1)\n",
    "    model = LinearRegression(n_jobs=4).fit(X, Y)\n",
    "    trend = model.predict(np.arange(len(metrics)).reshape(-1, 1))\n",
    "    detrend_metrics = metrics - trend\n",
    "    # detrend_metrics = metrics\n",
    "    return detrend_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def z_score_anomlay_detection(detrend_metrics: pd.DataFrame, start_min, end_min, y_pool, ignored_x, thd=3, percent=0.5):\n",
    "    def endswith_in(col, columns):\n",
    "        for c in columns:\n",
    "            if c.endswith(col):\n",
    "                return c\n",
    "        return False\n",
    "\n",
    "    detrend_m_zscore = detrend_metrics.loc[:, (detrend_metrics[start_min: end_min].abs() >= thd).sum(axis=0) / len(detrend_metrics[start_min: end_min]) >= percent]\n",
    "    flag = -1\n",
    "    y_col = endswith_in(y_pool[0], detrend_metrics.columns)\n",
    "    for i, col in enumerate(y_pool):\n",
    "        c = endswith_in(col, detrend_m_zscore.columns)\n",
    "        if c:\n",
    "            flag = i\n",
    "            y_col = c\n",
    "            print(\"{} is selected as y\".format(y_col))\n",
    "            break\n",
    "    for col in y_pool[flag + 1:]:\n",
    "        c = endswith_in(col, detrend_m_zscore.columns)\n",
    "        if c:\n",
    "            # print(\"drop {}\".format(c))\n",
    "            detrend_m_zscore.drop(c, axis=1, inplace=True)\n",
    "    for suffix in ignored_x:\n",
    "        for col in detrend_m_zscore.columns:\n",
    "            # if col.endswith(suffix):\n",
    "            if suffix in col:\n",
    "                # print(\"drop {}\".format(col))\n",
    "                detrend_m_zscore.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    detrend_m_zscore.loc[:, y_col] = detrend_metrics[y_col]\n",
    "\n",
    "    return detrend_m_zscore, y_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_column_pool = ['StreamingMetrics_streaming_lastCompletedBatch_processingDelay_value',\n",
    "                 'StreamingMetrics_streaming_lastCompletedBatch_schedulingDelay_value',\n",
    "                 'StreamingMetrics_streaming_lastCompletedBatch_totalDelay_value',\n",
    "\n",
    "                 ]\n",
    "\n",
    "\n",
    "diff_columns_contains = [\"StreamingMetrics_streaming_totalProcessedRecords_value\",\n",
    "                         \"StreamingMetrics_streaming_totalReceivedRecords_value\",\n",
    "                         'driver_BlockManager_memory_memUsed_MB_value'\n",
    "                         ]\n",
    "\n",
    "ignored_contains = ['Idle%', 'Sys%', 'Wait%', 'driver_LiveListenerBus', 'driver_DAGScheduler_messageProcessingTime', \"CodeGenerator\", \"driver_DAGScheduler\",\n",
    "                    'StreamingMetrics_streaming_runningBatches_value',\n",
    "                 'StreamingMetrics_streaming_unprocessedBatches_value',\n",
    "                 'StreamingMetrics_streaming_waitingBatches_value',\n",
    "                 'StreamingMetrics_streaming_retainedCompletedBatches_value',\n",
    "                 'StreamingMetrics_streaming_totalCompletedBatches_value',\n",
    "                    'StreamingMetrics_streaming_lastReceivedBatch_processingEndTime_value',\n",
    "                     'StreamingMetrics_streaming_lastReceivedBatch_processingStartTime_value',\n",
    "                    #  'driver_DAGScheduler_stage_waitingStages_value',\n",
    "                    # 'driver_DAGScheduler_stage_runningStages_value',\n",
    "                    'driver_BlockManager_memory_remainingMem_MB_value',\n",
    "                    'driver_BlockManager_memory_remainingOnHeapMem_MB_value',\n",
    "                    # 'driver_BlockManager_memory_memUsed_MB_value',\n",
    "                    'driver_BlockManager_memory_onHeapMemUsed_MB_value',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_cases(cases, trace_type='all', method='BMFS', topK=5):\n",
    "    save_xy = False\n",
    "    results = []\n",
    "    available_num = 0\n",
    "    hits = 0\n",
    "    rec_num = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, case in enumerate(cases):\n",
    "\n",
    "        if case['trace_type'] == 'process_failure' or case['anomaly_type'] == 'unknown' or case['anomaly_details'] == 'no_application_impact':\n",
    "            continue\n",
    "        if trace_type != 'all' and trace_type not in case['trace_type']:\n",
    "            continue\n",
    "        print(\"case index\", i)\n",
    "        print(\"trace_name: {}, anomaly_type: {}, anomaly_details: {}\".format(case['trace_name'], case['anomaly_type'], case['anomaly_details']))\n",
    "        available_num += 1\n",
    "\n",
    "        # preprocessing\n",
    "        norm_m, start_min, end_min = preprocessing(case, diff_columns_contains, pre='60min', after='5min')\n",
    "        detrend_m = detrending(norm_m, start_min=start_min)\n",
    "        detrend_m_zscore, y_col = z_score_anomlay_detection(detrend_m, start_min, end_min, y_column_pool, ignored_contains, thd=3, percent=0.5)\n",
    "\n",
    "        # X, y\n",
    "        print(y_col)\n",
    "        X: pd.DataFrame = detrend_m_zscore.drop(y_col, axis=1)\n",
    "        Y: pd.DataFrame = detrend_m_zscore[[y_col]]\n",
    "        y: pd.Series = detrend_m_zscore[y_col]\n",
    "        print(\"X shape:\", X.shape)\n",
    "        print(\"y shape:\", y.shape)\n",
    "        # X = X + 0.1 * np.random.randn(*X.shape)\n",
    "\n",
    "        # save X,y data\n",
    "        if save_xy:\n",
    "            X.to_csv(\"detrend_normalized_anomaly_data/case{}_X.csv\".format(i), index=True)\n",
    "            Y.to_csv(\"detrend_normalized_anomaly_data/case{}_Y.csv\".format(i), index=True)\n",
    "\n",
    "        # if X.shape[1] < 5:\n",
    "        #     beta_est = np.ones(X.shape[1])\n",
    "        if method == 'BMFS':\n",
    "            beta_est = bayesFS(X, y, positive=False, tol=1e-2, tol_ll=1e-3).coef_\n",
    "        elif method == 'ARD':\n",
    "            beta_est = my_ard_train(X, y, positive=False).coef_\n",
    "        elif method == 'E-Net':\n",
    "            beta_est = my_enet_train(X, y, pos=False).coef_\n",
    "        elif method == 'Lasso':\n",
    "            beta_est = my_lasso_train(X, y, pos=False).coef_\n",
    "        elif method == 'fsMTS':\n",
    "            beta_est = fsMTS_train(X, y).coef_\n",
    "        else:\n",
    "            raise ValueError(\"method should be BMFS or ard or enet or lasso or fsMTS\")\n",
    "\n",
    "        # compute attribution for columns with non-zero coefs\n",
    "        mask = np.abs(beta_est) > 0\n",
    "        result = sorted([(rc, coef, attribution) for rc, coef, attribution in zip(X.columns[mask], beta_est[mask],\n",
    "                                                                     1 / (y.values[-1] - np.mean(y.values)) * beta_est[mask] * (X.loc[:,mask].iloc[-1] - X.loc[:start_min, mask].mean()).values) if abs(attribution) > 0], key=lambda x:abs(x[2]), reverse=True)\n",
    "\n",
    "        # result = sorted([(rc, coef, attribution) for rc, coef, attribution in zip(X.columns[mask], beta_est[mask], beta_est[mask]) if abs(attribution) > 0], key=lambda x:abs(x[2]), reverse=True)\n",
    "\n",
    "        results.append(result)\n",
    "        result_cols = [rc for rc,_, _ in result[:topK]]\n",
    "        print([(rc, att) for rc,_, att in result[:topK]])\n",
    "        rec_num += len(result_cols)\n",
    "\n",
    "        # root cause matching\n",
    "        cpu_user_num = {}\n",
    "        task_num = 0\n",
    "        flow_num = 0\n",
    "        flow_direction = 1\n",
    "\n",
    "        for rc in result_cols:\n",
    "            if 'CPU' in rc:\n",
    "                node = rc.split('_')[0]\n",
    "                cpu_user_num[node] = cpu_user_num.get(node, 0) + 1\n",
    "            elif 'StreamingMetrics_streaming_lastReceivedBatch_records_value' in rc or 'StreamingMetrics_streaming_totalProcessedRecords_value' in rc or\\\n",
    "                    \"StreamingMetrics_streaming_totalReceivedRecords_value\" in rc or 'driver_BlockManager_memory_memUsed_MB_value' in rc:\n",
    "                flow_num += 1\n",
    "                if X.iloc[-1][rc] < 0:\n",
    "                    flow_direction = -1\n",
    "                else:\n",
    "                    flow_direction = 1\n",
    "            elif \"executor_threadpool_activeTasks_value\" in rc:\n",
    "                task_num += 1\n",
    "\n",
    "        if flow_num > 0:\n",
    "            if flow_direction == 1:\n",
    "                conclusion = 'bursty_input'\n",
    "                print(\"Conclusion: bursty_input from input records\")\n",
    "            else:\n",
    "                conclusion = 'stalled_input'\n",
    "                print(\"Conclusion: stalled_input from input records\")\n",
    "        elif len(cpu_user_num)==1:\n",
    "            node, count = [(key, count) for key, count in cpu_user_num.items()][0]\n",
    "            conclusion = 'cpu_contention'\n",
    "            print(\"Conclusion: cpu contention from {}\".format(node))\n",
    "            # if count > 1 or task_num==0:\n",
    "            #     conclusion = 'cpu_contention'\n",
    "            #     print(\"Conclusion: cpu contention from {}\".format(node))\n",
    "            # else:\n",
    "            #     conclusion = 'bursty_input'\n",
    "            #     print(\"Conclusion: bursty_input from active tasks and no cpu concentration\")\n",
    "\n",
    "        elif task_num > 0:\n",
    "            conclusion = 'bursty_input'\n",
    "            print(\"Conclusion: bursty_input from active tasks and no cpu concentration\")\n",
    "\n",
    "        else:\n",
    "            conclusion = 'unknown'\n",
    "            print(\"Conclusion: unknown\")\n",
    "\n",
    "        if conclusion in case['anomaly_type']:\n",
    "            hits += 1\n",
    "            print(\"hits: {}\".format(conclusion))\n",
    "\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "    return hits, available_num - hits, round(hits / available_num, 4), round((time.time() - t0) / available_num, 2), round(rec_num / available_num,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def results2table(results):\n",
    "    \"\"\"print result in a latex table style\"\"\"\n",
    "    for trace_type, result in results.items():\n",
    "        print(trace_type)\n",
    "        for method, perfs in result.items():\n",
    "            print(f\"{method}\\t &{perfs[0]}\\t &{perfs[1]}\\t &{perfs[2]}\\t &{perfs[3]}\\t &{perfs[4]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "# for trace_type in ['bursty_input', 'stalled_input', 'cpu_contention']:\n",
    "for trace_type in ['all']:\n",
    "    result = {}\n",
    "    for method in ['BMFS']:\n",
    "        res = evaluate_cases(cases, trace_type=trace_type, method=method)\n",
    "        result[method] = res\n",
    "    results[trace_type] = result\n",
    "\n",
    "results2table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results2table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for trace_type in ['bursty_input', 'stalled_input', 'cpu_contention', 'all']:\n",
    "# for trace_type in ['bursty_input']:\n",
    "    result = {}\n",
    "    for method in ['fsMTS']:\n",
    "        res = evaluate_cases(cases, trace_type=trace_type, method=method)\n",
    "        result[method] = res\n",
    "    results[trace_type] = result\n",
    "\n",
    "results2table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for trace_type in ['bursty_input', 'stalled_input', 'cpu_contention', 'all']:\n",
    "\n",
    "    result = {}\n",
    "    for method in ['E-Net']:\n",
    "        res = evaluate_cases(cases, trace_type=trace_type, method=method)\n",
    "        result[method] = res\n",
    "    results[trace_type] = result\n",
    "results2table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results2table(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('res_top5_bmfs_ranking_fsmts', 'r') as f:\n",
    "    df = pd.DataFrame(json.load(f))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
