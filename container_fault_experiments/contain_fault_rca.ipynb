{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from c_trainer import bayesFS, my_ard_train, my_enet_train, my_lasso_train, fsMTS_train\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"container_fault_dataset.csv\")\n",
    "containers = list(data.groupby(by=[\"container\"]).groups.keys())\n",
    "\n",
    "\n",
    "### standardize data\n",
    "def standardize_np(x):\n",
    "    EPS = 1e-16\n",
    "    x = (x - np.mean(x, 0)) / np.std(x, 0)\n",
    "    return x\n",
    "\n",
    "\n",
    "ard_times = []\n",
    "lasso_times = []\n",
    "enet_times = []\n",
    "bmfs_times = []\n",
    "fsMTS_times = []\n",
    "\n",
    "with open(\"label.json\", 'r') as fp:\n",
    "    label_dict = json.load(fp)\n",
    "ard_performance = {\"TP\": {}, \"FN\": {}, \"FP\": {}}\n",
    "lasso_performance = {\"TP\": {}, \"FN\": {}, \"FP\": {}}\n",
    "enet_performance = {\"TP\": {}, \"FN\": {}, \"FP\": {}}\n",
    "bmfs_performance = {\"TP\": {}, \"FN\": {}, \"FP\": {}}\n",
    "fsmts_performance = {\"TP\": {}, \"FN\": {}, \"FP\": {}}\n",
    "\n",
    "# run attribution for each container fault\n",
    "for container in containers:\n",
    "    single_container_data = data[data[\"container\"] == container]\n",
    "    select_cols = [\"cpu_util\", \"cpu_sys\", \"cpu_user\", \"mem_util\", \"mem_used\", \\\n",
    "                   \"tcp_currestab\", \"traffic_pkg_in\", \"traffic_pkg_out\", \\\n",
    "                   \"traffic_byte_in\", \"traffic_byte_out\"]\n",
    "    x = single_container_data[select_cols]\n",
    "    y = single_container_data[\"y\"]\n",
    "\n",
    "    ard_time_s = time.time()\n",
    "    ard = my_ard_train(x, y, False, \"standard\")\n",
    "    ard_times.append(time.time() - ard_time_s)\n",
    "\n",
    "    # run ARD regression and get the attribution score\n",
    "    print(container, \"ARD coef:\")\n",
    "    for j, metric in enumerate(select_cols):\n",
    "        print(metric, ard.coef_[j])\n",
    "        if label_dict[container][j] == 0 and ard.coef_[j] == 0:\n",
    "            continue\n",
    "        if metric not in ard_performance[\"TP\"]:\n",
    "            ard_performance[\"TP\"][metric] = 0\n",
    "        if metric not in ard_performance[\"FN\"]:\n",
    "            ard_performance[\"FN\"][metric] = 0\n",
    "        if metric not in ard_performance[\"FP\"]:\n",
    "            ard_performance[\"FP\"][metric] = 0\n",
    "        if label_dict[container][j] == 1 and abs(ard.coef_[j]) > 0:\n",
    "            ard_performance[\"TP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 0 and abs(ard.coef_[j]) > 0:\n",
    "            ard_performance[\"FP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 1 and abs(ard.coef_[j]) == 0:\n",
    "            ard_performance[\"FN\"][metric] += 1\n",
    "    print()\n",
    "\n",
    "    # run LASSO regression and get the attribution score\n",
    "    lasso_time_s = time.time()\n",
    "    lasso = my_lasso_train(x, y, False, \"standard\")\n",
    "    lasso_times.append(time.time() - lasso_time_s)\n",
    "    print(container, \"LASSO coef:\")\n",
    "    for j, metric in enumerate(select_cols):\n",
    "        print(metric, lasso.coef_[j])\n",
    "        if label_dict[container][j] == 0 and lasso.coef_[j] == 0:\n",
    "            continue\n",
    "        if metric not in lasso_performance[\"TP\"]:\n",
    "            lasso_performance[\"TP\"][metric] = 0\n",
    "        if metric not in lasso_performance[\"FN\"]:\n",
    "            lasso_performance[\"FN\"][metric] = 0\n",
    "        if metric not in lasso_performance[\"FP\"]:\n",
    "            lasso_performance[\"FP\"][metric] = 0\n",
    "        if label_dict[container][j] == 1 and abs(lasso.coef_[j]) > 0:\n",
    "            lasso_performance[\"TP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 0 and abs(lasso.coef_[j]) > 0:\n",
    "            lasso_performance[\"FP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 1 and abs(lasso.coef_[j]) == 0:\n",
    "            lasso_performance[\"FN\"][metric] += 1\n",
    "    print()\n",
    "\n",
    "    # run Elastic Net regression and get the attribution score\n",
    "    enet_time_s = time.time()\n",
    "    enet = my_enet_train(x, y, False, \"standard\")\n",
    "    enet_times.append(time.time() - enet_time_s)\n",
    "    print(container, \"Elastic Net coef:\")\n",
    "    for j, metric in enumerate(select_cols):\n",
    "        print(metric, enet.coef_[j])\n",
    "        if label_dict[container][j] == 0 and enet.coef_[j] == 0:\n",
    "            continue\n",
    "        if metric not in enet_performance[\"TP\"]:\n",
    "            enet_performance[\"TP\"][metric] = 0\n",
    "        if metric not in enet_performance[\"FN\"]:\n",
    "            enet_performance[\"FN\"][metric] = 0\n",
    "        if metric not in enet_performance[\"FP\"]:\n",
    "            enet_performance[\"FP\"][metric] = 0\n",
    "        if label_dict[container][j] == 1 and abs(enet.coef_[j]) > 0:\n",
    "            enet_performance[\"TP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 0 and abs(enet.coef_[j]) > 0:\n",
    "            enet_performance[\"FP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 1 and abs(enet.coef_[j]) == 0:\n",
    "            enet_performance[\"FN\"][metric] += 1\n",
    "    print()\n",
    "\n",
    "    # run BMFS and get the attribution score\n",
    "    bmfs_time_s = time.time()\n",
    "    BMFS = bayesFS(x, y, False, \"standard\")\n",
    "    bmfs_times.append(time.time() - bmfs_time_s)\n",
    "    print(container, \"BMFS coef:\")\n",
    "    for j, metric in enumerate(select_cols):\n",
    "        print(metric, BMFS.coef_[j])\n",
    "        if label_dict[container][j] == 0 and BMFS.coef_[j] == 0:\n",
    "            continue\n",
    "        if metric not in bmfs_performance[\"TP\"]:\n",
    "            bmfs_performance[\"TP\"][metric] = 0\n",
    "        if metric not in bmfs_performance[\"FN\"]:\n",
    "            bmfs_performance[\"FN\"][metric] = 0\n",
    "        if metric not in bmfs_performance[\"FP\"]:\n",
    "            bmfs_performance[\"FP\"][metric] = 0\n",
    "        if label_dict[container][j] == 1 and abs(BMFS.coef_[j]) > 0:\n",
    "            bmfs_performance[\"TP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 0 and abs(BMFS.coef_[j]) > 0:\n",
    "            bmfs_performance[\"FP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 1 and abs(BMFS.coef_[j]) == 0:\n",
    "            bmfs_performance[\"FN\"][metric] += 1\n",
    "    print()\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    # run fsMTS and get the attribution score\n",
    "    fsMTS_time_s = time.time()\n",
    "    fsMTS = fsMTS_train(x, y, \"standard\")\n",
    "    fsMTS_times.append(time.time() - fsMTS_time_s)\n",
    "    print(container, \"FSMTS coef:\")\n",
    "    for j, metric in enumerate(select_cols):\n",
    "        print(metric, fsMTS.coef_[j])\n",
    "        if label_dict[container][j] == 0 and fsMTS.coef_[j] == 0:\n",
    "            continue\n",
    "        if metric not in fsmts_performance[\"TP\"]:\n",
    "            fsmts_performance[\"TP\"][metric] = 0\n",
    "        if metric not in fsmts_performance[\"FN\"]:\n",
    "            fsmts_performance[\"FN\"][metric] = 0\n",
    "        if metric not in fsmts_performance[\"FP\"]:\n",
    "            fsmts_performance[\"FP\"][metric] = 0\n",
    "        if label_dict[container][j] == 1 and abs(fsMTS.coef_[j]) > 0:\n",
    "            fsmts_performance[\"TP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 0 and abs(fsMTS.coef_[j]) > 0:\n",
    "            fsmts_performance[\"FP\"][metric] += 1\n",
    "        elif label_dict[container][j] == 1 and abs(fsMTS.coef_[j]) == 0:\n",
    "            fsmts_performance[\"FN\"][metric] += 1\n",
    "    print()\n",
    "\n",
    "    print()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARD Average Time 0.40609402656555177\n",
      "LASSO Average Time 0.5600771474838256\n",
      "Elastic Net Average Time 0.5998151302337646\n",
      "BMFS Average Time 0.20007145166397095\n",
      "FSMTS Average Time 0.89454594373703\n",
      "\n",
      "ARD precision, recall, and f1 score: 0.9803665158371041 0.4783908045977011 0.6266195620660145\n",
      "LASSO precision, recall, and f1 score: 0.5213130277622099 0.5090700104493209 0.47434082434082436\n",
      "Enet precision, recall, and f1 score: 0.5528071087409323 0.7352246603970742 0.5933715607127658\n",
      "BMFS precision, recall, and f1 score: 0.9134323305212545 0.8145977011494253 0.854825938969767\n",
      "fsMTS precision, recall, and f1 score: 0.5201816405998592 0.4808986415882968 0.4585272851890423\n"
     ]
    }
   ],
   "source": [
    "# show time complexity for each method\n",
    "print(\"ARD Average Time\", np.mean(ard_times))\n",
    "print(\"LASSO Average Time\", np.mean(lasso_times))\n",
    "print(\"Elastic Net Average Time\", np.mean(enet_times))\n",
    "print(\"BMFS Average Time\", np.mean(bmfs_times))\n",
    "print(\"FSMTS Average Time\", np.mean(fsMTS_times))\n",
    "print()\n",
    "\n",
    "# compute recall, precision, and f1 score for each method over the container fault dataset\n",
    "ard_precisions = []\n",
    "ard_recalls = []\n",
    "ard_f1_scores = []\n",
    "for metric in ard_performance[\"TP\"]:\n",
    "    ard_precisions.append(\n",
    "        ard_performance[\"TP\"][metric] / (ard_performance[\"TP\"][metric] + ard_performance[\"FP\"][metric]))\n",
    "    ard_recalls.append(ard_performance[\"TP\"][metric] / (ard_performance[\"TP\"][metric] + ard_performance[\"FN\"][metric]))\n",
    "    if ard_precisions[-1] == 0 and ard_recalls[-1] == 0:\n",
    "        ard_f1_scores.append(0)\n",
    "        continue\n",
    "    ard_f1_scores.append(2 * ard_precisions[-1] * ard_recalls[-1] / (ard_precisions[-1] + ard_recalls[-1]))\n",
    "print(\"ARD precision, recall, and f1 score:\", np.mean(ard_precisions), np.mean(ard_recalls), np.mean(ard_f1_scores))\n",
    "\n",
    "lasso_precisions = []\n",
    "lasso_recalls = []\n",
    "lasso_f1_scores = []\n",
    "for metric in lasso_performance[\"TP\"]:\n",
    "    lasso_precisions.append(\n",
    "        lasso_performance[\"TP\"][metric] / (lasso_performance[\"TP\"][metric] + lasso_performance[\"FP\"][metric]))\n",
    "    lasso_recalls.append(\n",
    "        lasso_performance[\"TP\"][metric] / (lasso_performance[\"TP\"][metric] + lasso_performance[\"FN\"][metric]))\n",
    "    if lasso_precisions[-1] == 0 and lasso_recalls[-1] == 0:\n",
    "        lasso_f1_scores.append(0)\n",
    "        continue\n",
    "    lasso_f1_scores.append(2 * lasso_precisions[-1] * lasso_recalls[-1] / (lasso_precisions[-1] + lasso_recalls[-1]))\n",
    "print(\"LASSO precision, recall, and f1 score:\", np.mean(lasso_precisions), np.mean(lasso_recalls),\n",
    "      np.mean(lasso_f1_scores))\n",
    "\n",
    "enet_precisions = []\n",
    "enet_recalls = []\n",
    "enet_f1_scores = []\n",
    "for metric in enet_performance[\"TP\"]:\n",
    "    enet_precisions.append(\n",
    "        enet_performance[\"TP\"][metric] / (enet_performance[\"TP\"][metric] + enet_performance[\"FP\"][metric]))\n",
    "    enet_recalls.append(\n",
    "        enet_performance[\"TP\"][metric] / (enet_performance[\"TP\"][metric] + enet_performance[\"FN\"][metric]))\n",
    "    if enet_precisions[-1] == 0 and enet_recalls[-1] == 0:\n",
    "        enet_f1_scores.append(0)\n",
    "        continue\n",
    "    enet_f1_scores.append(2 * enet_precisions[-1] * enet_recalls[-1] / (enet_precisions[-1] + enet_recalls[-1]))\n",
    "print(\"Enet precision, recall, and f1 score:\", np.mean(enet_precisions), np.mean(enet_recalls), np.mean(enet_f1_scores))\n",
    "\n",
    "bmfs_precisions = []\n",
    "bmfs_recalls = []\n",
    "bmfs_f1_scores = []\n",
    "for metric in bmfs_performance[\"TP\"]:\n",
    "    bmfs_precisions.append(\n",
    "        bmfs_performance[\"TP\"][metric] / (bmfs_performance[\"TP\"][metric] + bmfs_performance[\"FP\"][metric]))\n",
    "    bmfs_recalls.append(\n",
    "        bmfs_performance[\"TP\"][metric] / (bmfs_performance[\"TP\"][metric] + bmfs_performance[\"FN\"][metric]))\n",
    "    if bmfs_precisions[-1] == 0 and bmfs_recalls[-1] == 0:\n",
    "        bmfs_f1_scores.append(0)\n",
    "        continue\n",
    "    bmfs_f1_scores.append(2 * bmfs_precisions[-1] * bmfs_recalls[-1] / (bmfs_precisions[-1] + bmfs_recalls[-1]))\n",
    "print(\"BMFS precision, recall, and f1 score:\", np.mean(bmfs_precisions), np.mean(bmfs_recalls), np.mean(bmfs_f1_scores))\n",
    "\n",
    "fsmts_precisions = []\n",
    "fsmts_recalls = []\n",
    "fsmts_f1_scores = []\n",
    "for metric in fsmts_performance[\"TP\"]:\n",
    "    if fsmts_performance[\"TP\"][metric] == 0:\n",
    "        fsmts_precisions.append(0)\n",
    "        fsmts_recalls.append(0)\n",
    "    else:\n",
    "        fsmts_precisions.append(\n",
    "            fsmts_performance[\"TP\"][metric] / (fsmts_performance[\"TP\"][metric] + fsmts_performance[\"FP\"][metric]))\n",
    "        fsmts_recalls.append(\n",
    "            fsmts_performance[\"TP\"][metric] / (fsmts_performance[\"TP\"][metric] + fsmts_performance[\"FN\"][metric]))\n",
    "    if fsmts_precisions[-1] == 0 and fsmts_recalls[-1] == 0:\n",
    "        fsmts_f1_scores.append(0)\n",
    "        continue\n",
    "    fsmts_f1_scores.append(2 * fsmts_precisions[-1] * fsmts_recalls[-1] / (fsmts_precisions[-1] + fsmts_recalls[-1]))\n",
    "print(\"fsMTS precision, recall, and f1 score:\", np.mean(fsmts_precisions), np.mean(fsmts_recalls),\n",
    "      np.mean(fsmts_f1_scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}